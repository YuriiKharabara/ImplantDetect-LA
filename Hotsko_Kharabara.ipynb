{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import cv2 \n",
    "import tensorflow as tf\n",
    "import pydicom\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def dicom_to_jpeg(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.dcm'):\n",
    "            dicom_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            dicom_image = pydicom.dcmread(dicom_path).pixel_array\n",
    "\n",
    "            # Normalize to 0-255 and convert to 8-bit data\n",
    "            dicom_image = np.uint8((dicom_image / np.max(dicom_image)) * 255)\n",
    "\n",
    "            im = Image.fromarray(dicom_image)\n",
    "\n",
    "            jpeg_path = os.path.join(\n",
    "                output_folder, filename.replace('.dcm', '.jpg'))\n",
    "\n",
    "            im.save(jpeg_path)\n",
    "\n",
    "\n",
    "dicom_to_jpeg('data/dataset_another/DICOM', 'data/dataset_another/jpgs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_data(path_to_file):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and returns a list of lists containing the data\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    for i in range(1, len(data)):\n",
    "        csv_data = data[i].split(',')\n",
    "        y_value = 1 if csv_data[1] == 'abnormal\\n' else 0\n",
    "        X.append(csv_data[0])\n",
    "        Y.append(y_value)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_info = read_csv_data('data/dataset_another/all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_train_images(folder_path, csv_info):\n",
    "    \"\"\"\n",
    "    Reads all the images in the folder and returns a list of images\n",
    "    \"\"\"\n",
    "    num_files = len(csv_info[0])\n",
    "    images_np = np.zeros((num_files, 100, 156), dtype=np.uint8)\n",
    "    valid_indices = [] \n",
    "\n",
    "    for i, filename in enumerate(csv_info[0]):\n",
    "        img_path = os.path.join(folder_path, f\"{filename}.jpg\")\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (156, 100))\n",
    "        images_np[i] = img\n",
    "        img = img.flatten()\n",
    "        if np.std(img) != 0:\n",
    "            valid_indices.append(i)\n",
    "    print(f\"Number of valid images: {len(valid_indices)}\")\n",
    "    print(len(images_np))\n",
    "    X = images_np[valid_indices]\n",
    "    Y = csv_info[1][valid_indices]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read_all_train_images('data/dataset_another/jpgs', csv_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, h, w, rows=3, cols=4):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    n_images = rows * cols\n",
    "\n",
    "    for i in range(n_images):\n",
    "        if i < len(images):\n",
    "            axes[i].imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "            axes[i].set_title(i, fontsize=12)\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "    # Remove empty subplots\n",
    "    while i < n_images - 1:\n",
    "        i += 1\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_images(x_test, 100, 156)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    mean_of_each_image = data.mean(axis=0)\n",
    "    data_normalized = data - mean_of_each_image\n",
    "    # data_normalized /= np.std(data, axis=0)\n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, h, w, approximation=0.83):\n",
    "    # normalize the data\n",
    "    data_normalized = normalize_data(data)\n",
    "    #Find the covariance matrix\n",
    "    print(\"Step 1\")\n",
    "    covariance_matrix = np.cov(data_normalized, rowvar=False)\n",
    "    #Find the eigenvalues and eigenvectors of the covariance matrix\n",
    "    print(\"Step 2\")\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n",
    "    print(\"Step 3\")\n",
    "    #Sort the eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "    print(\"Step 4\")\n",
    "\n",
    "    #Find the number of principal components that explain the given approximation\n",
    "    total_variance = np.sum(eigenvalues)\n",
    "    variance_explained = eigenvalues / total_variance\n",
    "    cumulative_variance_explained = np.cumsum(variance_explained)\n",
    "    number_of_components = np.argmax(cumulative_variance_explained >= approximation) + 1\n",
    "    #Find the principal components\n",
    "    print(\"Step 6\")\n",
    "    principal_components = eigenvectors[:, :number_of_components]\n",
    "    return principal_components, data_normalized, number_of_components, cumulative_variance_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(data, principal_components):\n",
    "    data_normalized = normalize_data(data)\n",
    "    return data_normalized.dot(principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components, data_normalized, number_of_components, cumulative_variance_explained = pca(x_train, 100, 100, 0.83)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_normalized_reduced = transform_data(x_train, principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_normalized_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_normalized_reduced.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_explained(cumulative_variance_explained, number_of_components):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(1, number_of_components + 1), cumulative_variance_explained[:number_of_components], '-o')\n",
    "    plt.xticks(np.arange(1, number_of_components + 1))\n",
    "    plt.xlabel('Number of Principal Components', fontsize=12)\n",
    "    plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "    plt.title('Variance Explained by Principal Components', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "plot_variance_explained(cumulative_variance_explained, number_of_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components(principal_components, im_shape=(100, 100), n_row=10, n_col=None):\n",
    "    if n_col is None:\n",
    "        n_col = principal_components.shape[1] // n_row\n",
    "\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, axi in enumerate(axes):\n",
    "        if i < principal_components.shape[1]:\n",
    "            img = principal_components[:, i].reshape(im_shape)\n",
    "            axi.imshow(img, cmap=\"gray\")\n",
    "            axi.set_xlabel(f\"PC{i+1}\")\n",
    "            axi.set_xticks([])\n",
    "            axi.set_yticks([])\n",
    "        else:\n",
    "            axi.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_principal_components(principal_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_normalized_reduced_x_test = transform_data(x_test, principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_normalized_reduced_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn classify\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Reshape((number_of_components, 1), input_shape=(number_of_components,)),\n",
    "    tf.keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(final_data_normalized_reduced, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(final_data_normalized_reduced_x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predicted, true):\n",
    "    sum = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == true[i]:\n",
    "            sum+=1\n",
    "    return sum/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dtc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train, y_train)\n",
    "y_pred = rfc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train, y_train)\n",
    "y_pred = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dental_base.csv\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    with open(\"dental_base1.csv\", \"w\") as file1:\n",
    "        for i in lines:\n",
    "            line = i[:-1].split(\";\")\n",
    "            if line[1] == '':\n",
    "                line[1] = '0'\n",
    "            line_new = f\"{line[0]};{line[1]}\\n\"\n",
    "            \n",
    "            file1.write(line_new)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
